{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "def get_spectrograms(file_name):\n",
    "    audio, sr = librosa.load(file_name, sr=None, mono=None)\n",
    "    audio = audio.mean(axis=0)\n",
    "    magnitude_spectrogram = np.abs(librosa.stft(audio, n_fft=1024, hop_length=512))\n",
    "    magnitude_spectrogram = magnitude_spectrogram[:, 0:800] # cropping the time dimension\n",
    "    log_spectrogram = librosa.amplitude_to_db(magnitude_spectrogram)\n",
    "    log_spectrogram_normalized = scaler.fit_transform(log_spectrogram)\n",
    "    log_spectrogram_tensor = torch.tensor(log_spectrogram_normalized, dtype=torch.float32)\n",
    "    return log_spectrogram_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 513, 800)\n"
     ]
    }
   ],
   "source": [
    "# Getting 300 spectrograms to train the diffusion autoencoder\n",
    "spectrograms = []\n",
    "i = 0\n",
    "for file in os.listdir('./train_data'):\n",
    "\n",
    "    spectrograms.append(get_spectrograms(os.path.join('./train_data',file)))\n",
    "    i += 1\n",
    "\n",
    "spectrograms = np.array(spectrograms)\n",
    "print(spectrograms.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramAutoencoder(nn.Module):\n",
    "    def __init__(self, freq_bins, time_stamps, latent_dim):\n",
    "        super(SpectrogramAutoencoder, self).__init__()\n",
    "        \n",
    "        self.freq_bins = freq_bins\n",
    "        self.time_stamps = time_stamps\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # (1, F, T) -> (32, F, T)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),      # -> (32, F/2, T/2)\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # (32, F/2, T/2) -> (64, F/2, T/2)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),      # -> (64, F/4, T/4)\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),# (64, F/4, T/4) -> (128, F/4, T/4)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)       # -> (128, F/8, T/8)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.encoder_output_size=819200 # found out after checking the dimensions of input after self.flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(self.encoder_output_size, latent_dim)\n",
    "        \n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.fc2 = nn.Linear(latent_dim, self.encoder_output_size)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), # (128, F/8, T/8) -> (128, F/8, T/8)\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),             # -> (128, F/4, T/4)\n",
    "            \n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),  # (128, F/4, T/4) -> (64, F/4, T/4)\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),             # -> (64, F/2, T/2)\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),   # (64, F/2, T/2) -> (32, F/2, T/2)\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),             # -> (32, F, T)\n",
    "            \n",
    "            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=(1, 0)),    # (32, F, T) -> (1, F, T)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input for convolutional layers\n",
    "        x = x.unsqueeze(1)  # Add channel dimension: (N, 1, F, T)\n",
    "        \n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        # print(x.shape, \"after encoder\")\n",
    "        x = self.flatten(x)\n",
    "        # print(x.shape, \"after flattening\")\n",
    "        latent = self.fc1(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.fc2(latent)\n",
    "        x = x.view(-1, 128, self.freq_bins // 8, self.time_stamps // 8)\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        # print(x.shape, 'shape after decoding')\n",
    "        x = x.squeeze(1)\n",
    "        # print(x.shape, 'shape after squeezing')\n",
    "        x = F.pad(x, (0, 2, 0, 1))\n",
    "        print(x.shape)\n",
    "        return x , latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([15, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([28, 513, 800])\n",
      "Epoch 1/5, Train Loss: 0.0749, Val Loss: 0.0324\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n",
      "torch.Size([32, 513, 800])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "freq_bins = 513\n",
    "time_stamps = 800\n",
    "latent_dim = 32\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model\n",
    "model = SpectrogramAutoencoder(freq_bins=freq_bins, time_stamps=time_stamps, latent_dim=latent_dim)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "train_size = int(0.8 * len(spectrograms))\n",
    "val_size = len(spectrograms) - train_size\n",
    "train_data, val_data = random_split(spectrograms, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        outputs, latent_space = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            outputs, _ = model(batch)\n",
    "            val_loss = criterion(outputs, batch)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    # Average losses\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a DataLoader for the spectrograms\n",
    "full_loader = DataLoader(spectrograms, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize a list to store latent representations\n",
    "latent_representations = []\n",
    "\n",
    "# Generate latent representations\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        batch = batch.to(device)\n",
    "        _, latent_space = model(batch)  # Get the latent representations\n",
    "        latent_representations.append(latent_space.cpu().numpy())  # Move to CPU and convert to numpy array\n",
    "\n",
    "# Concatenate all latent representations into a single array\n",
    "latent_representations = np.concatenate(latent_representations, axis=0)\n",
    "\n",
    "# Print the shape of the latent representations\n",
    "print(f\"Latent representations shape: {latent_representations.shape}\")\n",
    "\n",
    "# Save the latent representations to a file if needed\n",
    "np.save(\"latent_representations.npy\", latent_representations)\n",
    "\n",
    "print(\"Latent representations saved to 'latent_representations.npy'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
